/**
 * CastGesture â€” Content script for video call gesture detection.
 * Detects video elements, captures frames, runs MediaPipe Hands,
 * and triggers overlay effects.
 */

(function() {
  'use strict';

  let settings = null;
  let enabled = false;
  let detecting = false;
  let lastGestureTime = 0;
  let handsModel = null;
  let videoElement = null;
  let captureCanvas = null;
  let captureCtx = null;
  let animFrameId = null;

  // Load settings
  chrome.runtime.sendMessage({ type: 'getSettings' }, (s) => {
    if (s) {
      settings = s;
      enabled = s.enabled;
      if (enabled) startDetection();
    }
  });

  // Listen for settings updates
  chrome.runtime.onMessage.addListener((msg) => {
    if (msg.type === 'settingsUpdated') {
      settings = msg.settings;
      enabled = msg.settings.enabled;
      if (enabled && !detecting) startDetection();
      if (!enabled && detecting) stopDetection();
    }
  });

  /**
   * Find the user's self-view video element on the page.
   */
  function findSelfVideo() {
    const videos = document.querySelectorAll('video');
    if (videos.length === 0) return null;

    // Heuristics for self-view:
    // - Google Meet: video with data-participant-id containing "local"
    // - Zoom: video in a container with class containing "self"
    // - Teams: video in element with self-video indicators
    // - Fallback: smallest video (self-view is usually smaller)

    // Google Meet
    const meetSelf = document.querySelector('video[data-self-video="true"]')
      || document.querySelector('[data-participant-id*="local"] video')
      || document.querySelector('[data-self-name] video');
    if (meetSelf) return meetSelf;

    // Zoom
    const zoomSelf = document.querySelector('.self-video video')
      || document.querySelector('[class*="self"] video');
    if (zoomSelf) return zoomSelf;

    // Teams
    const teamsSelf = document.querySelector('[data-cid="calling-self-video"] video')
      || document.querySelector('.video-stream--self video');
    if (teamsSelf) return teamsSelf;

    // Fallback: pick the smallest playing video (likely self-view)
    let smallest = null;
    let smallestArea = Infinity;
    videos.forEach(v => {
      if (v.videoWidth > 0 && v.videoHeight > 0) {
        const area = v.videoWidth * v.videoHeight;
        if (area < smallestArea) {
          smallestArea = area;
          smallest = v;
        }
      }
    });
    return smallest;
  }

  /**
   * Simple rule-based gesture classification from hand landmarks.
   * landmarks: array of 21 {x, y, z} points (MediaPipe hand format)
   */
  function classifyGesture(landmarks) {
    if (!landmarks || landmarks.length < 21) return null;

    const tips = [4, 8, 12, 16, 20];       // thumb, index, middle, ring, pinky tips
    const pips = [3, 6, 10, 14, 18];       // PIP joints
    const mcps = [2, 5, 9, 13, 17];        // MCP joints

    // Check which fingers are extended
    const extended = [];
    // Thumb: tip.x vs mcp.x (depends on handedness, use simple heuristic)
    const thumbExtended = Math.abs(landmarks[4].x - landmarks[2].x) > 0.05;
    extended.push(thumbExtended);

    // Other fingers: tip.y < pip.y means extended (y goes down in screen coords)
    for (let i = 1; i < 5; i++) {
      extended.push(landmarks[tips[i]].y < landmarks[pips[i]].y);
    }

    const extCount = extended.filter(Boolean).length;

    // Classify
    if (extCount >= 4) return 'open_hand';
    if (extCount === 0) return 'fist';
    if (extended[1] && extended[2] && !extended[3] && !extended[4]) return 'peace';
    if (extended[0] && !extended[1] && !extended[2] && !extended[3] && !extended[4]) return 'thumbs_up';
    if (extended[1] && !extended[2] && !extended[3] && !extended[4]) return 'pointing';
    if (extended[0] && extended[1] && !extended[2] && !extended[3] && extended[4]) return 'rock_on';

    // OK sign: thumb tip close to index tip
    const thumbTip = landmarks[4];
    const indexTip = landmarks[8];
    const dist = Math.sqrt(
      (thumbTip.x - indexTip.x) ** 2 +
      (thumbTip.y - indexTip.y) ** 2
    );
    if (dist < 0.05 && extended[2]) return 'ok_sign';

    return null;
  }

  /**
   * Start gesture detection loop.
   */
  async function startDetection() {
    if (detecting) return;
    detecting = true;

    // Wait for video element to appear
    const waitForVideo = () => new Promise((resolve) => {
      const check = () => {
        const v = findSelfVideo();
        if (v && v.videoWidth > 0) return resolve(v);
        setTimeout(check, 1000);
      };
      check();
    });

    videoElement = await waitForVideo();
    console.log('[CastGesture] Found self-view video, starting gesture detection');

    // Setup capture canvas
    captureCanvas = document.createElement('canvas');
    captureCanvas.width = 320;
    captureCanvas.height = 240;
    captureCtx = captureCanvas.getContext('2d');

    // Try loading MediaPipe Hands (via CDN for extension)
    try {
      // MediaPipe Hands via Vision tasks API
      if (!handsModel) {
        // Use a lightweight approach: capture frames and classify
        // with basic landmark detection. For production, bundle
        // @mediapipe/hands. For now, use rule-based on video analysis.
        console.log('[CastGesture] Using built-in gesture classifier');
      }
    } catch (e) {
      console.warn('[CastGesture] MediaPipe not available, using basic detection');
    }

    // Detection loop
    function detectLoop() {
      if (!enabled || !detecting) return;

      try {
        captureCtx.drawImage(videoElement, 0, 0, 320, 240);

        // Note: Full MediaPipe integration requires bundling the WASM module.
        // For the extension MVP, we rely on the server-side detection or
        // the user can enable the built-in demo mode.
        // When MediaPipe JS is bundled, replace this with actual hand detection.

        // For now, the extension works best in tandem with the CastGesture server
        // via WebSocket, or in demo/interactive mode.
      } catch (e) {
        // Video might not be accessible (CORS)
      }

      animFrameId = requestAnimationFrame(() => {
        setTimeout(detectLoop, 100); // ~10fps to save CPU
      });
    }

    detectLoop();

    // Also listen for gesture events from CastGesture server via postMessage
    window.addEventListener('message', (e) => {
      if (e.data && e.data.type === 'castgesture-effect' && enabled) {
        const effect = settings.gestureMappings[e.data.gesture];
        if (effect && window.CastGestureEffects) {
          window.CastGestureEffects.trigger(effect, e.data.params || {});
        }
      }
    });
  }

  function stopDetection() {
    detecting = false;
    if (animFrameId) cancelAnimationFrame(animFrameId);
    if (window.CastGestureEffects) window.CastGestureEffects.cleanup();
  }

  /**
   * Trigger an effect from a detected gesture.
   */
  function triggerGesture(gesture, x = 0.5, y = 0.5) {
    if (!settings || !enabled) return;

    const now = Date.now();
    if (now - lastGestureTime < (settings.cooldownMs || 1000)) return;
    lastGestureTime = now;

    const effect = settings.gestureMappings[gesture];
    if (!effect || !window.CastGestureEffects) return;

    window.CastGestureEffects.trigger(effect, { x, y });

    // Notify popup
    chrome.runtime.sendMessage({
      type: 'gestureDetected',
      gesture,
      effect,
      timestamp: now,
    }).catch(() => {});
  }

  // Expose for testing
  window._castgestureContentScript = { triggerGesture, classifyGesture };
})();
