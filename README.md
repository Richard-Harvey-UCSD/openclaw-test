# GestureEngine ğŸ¤š

**Real-time hand gesture recognition for edge devices.** No cloud. No latency. Just hands.

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-green.svg)](https://python.org)

GestureEngine turns any RGB camera into a gesture input device. It runs on a Raspberry Pi, ships with a WebSocket streaming API, and recognizes both individual gestures and multi-gesture sequences â€” all in under 5ms per frame.

---

## âš¡ Quick Start

```bash
git clone https://github.com/yourorg/gesture-engine.git
cd gesture-engine
pip install -e ".[server]"

# Start the WebSocket server + web demo
python -m gesture_engine --port 8765

# Open http://localhost:8765 in your browser
```

## ğŸ¯ What It Does

| Feature | Description |
|---------|-------------|
| **7 built-in gestures** | open_hand, fist, thumbs_up, peace, pointing, rock_on, ok_sign |
| **6 gesture sequences** | release, grab, wave, peace_out, pinch_release, point_and_click |
| **WebSocket streaming** | Real-time gesture events pushed to any client |
| **Browser demo** | Dark-themed live UI with confidence meters and timeline |
| **Recording & replay** | Capture sessions for testing without a camera |
| **Benchmark suite** | Measure latency and throughput on your hardware |
| **Custom gestures** | Define via JSON or train an MLP classifier |

## ğŸ— Architecture

```
Camera Frame (RGB)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HandDetector â”‚  â† MediaPipe (21 3D landmarks)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Normalized landmarks (wrist-centered, scale-invariant)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GestureClassifier â”‚  â† Rule-based OR trained MLP (81-dim features)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â–¶ GesturePipeline  â† Temporal smoothing, cooldown, callbacks
       â”‚
       â””â”€â”€â–¶ SequenceDetector â† Multi-gesture pattern matching
              â”‚
              â–¼
         WebSocket Server  â† FastAPI/uvicorn â†’ Browser clients
```

### Design Decisions

- **Landmark-based, not pixel-based.** Classify hand geometry, not images. Makes the model tiny and position/scale invariant.
- **81-dimensional feature vector** â€” fingertip distances, extension ratios, palm orientation. Not just raw coordinates.
- **Temporal smoothing** via majority vote eliminates single-frame jitter.
- **Sequence detection** watches for gesture transitions within time windows (e.g., fistâ†’open_hand = "release").

## ğŸ–¥ WebSocket Streaming Server

The server captures from the webcam and pushes gesture events to all connected WebSocket clients:

```bash
# Start server
python -m gesture_engine --host 0.0.0.0 --port 8765

# Or via uvicorn directly
uvicorn gesture_engine.server:app --host 0.0.0.0 --port 8765
```

**Endpoints:**
- `GET /` â€” Web demo UI
- `GET /api/status` â€” Server stats (FPS, latency, clients)
- `GET /api/gestures` â€” Registered gesture definitions
- `WS /ws` â€” Real-time gesture event stream

**WebSocket message types:**
```json
{"type": "gesture", "gesture": "peace", "confidence": 0.95, "hand_index": 0, "timestamp": 1708000000.0}
{"type": "sequence", "sequence": "grab", "gestures": ["open_hand", "fist"], "duration": 0.8}
{"type": "stats", "fps": 28.5, "latency_ms": 35.1, "hands_detected": 1}
```

## ğŸŒ Browser Demo

Open `http://localhost:8765` after starting the server. Features:

- **Live gesture display** with emoji and confidence meter
- **Gesture sequence detection** highlighted in gold
- **Event timeline** with timestamps
- **Real-time metrics** â€” FPS, latency, hand count
- Dark theme, no JavaScript frameworks, pure CSS

## ğŸ¬ Recording & Replay

Record gesture sessions for reproducible testing:

```python
from gesture_engine import GestureRecorder, GesturePlayer

# Record
recorder = GestureRecorder()
recorder.start()
recorder.add_frame(hand_landmarks, [{"name": "peace", "confidence": 0.9}])
recorder.stop()
recorder.save("session.json")           # JSON format
recorder.save_compact("session.npz")    # Compact binary

# Replay
player = GesturePlayer.load("session.json")
for frame in player.play():             # Instant playback
    process(frame.hands)

for frame in player.play_realtime(speed=2.0):  # 2x speed
    process(frame.hands)
```

Useful for:
- CI pipelines on headless machines
- Demo recordings without camera access
- Regression testing

## ğŸ”— Gesture Sequences

Detect compound gestures â€” ordered transitions within a time window:

```python
from gesture_engine import SequenceDetector, GestureSequence

detector = SequenceDetector.with_defaults()

# Built-in sequences:
# fist â†’ open_hand    = "release"
# open_hand â†’ fist    = "grab"
# peace â†’ fist        = "peace_out"
# pointing â†’ fist     = "point_and_click"
# ok_sign â†’ open_hand = "pinch_release"
# open_hand â†’ fist â†’ open_hand = "wave"

# Custom sequences:
detector.register(GestureSequence(
    name="swipe_right",
    gestures=["pointing", "open_hand"],
    max_duration=1.0,
))

# Feed gesture observations
events = detector.feed("fist")
events = detector.feed("open_hand")  # â†’ triggers "release"
```

## ğŸ“Š Benchmarks

```bash
python examples/benchmark.py
python examples/benchmark.py --iterations 5000 --hands 2
```

Sample output (Raspberry Pi 4):

```
  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
  â”‚ Rule-Based Classification                        â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Mean latency         0.042 ms                    â”‚
  â”‚ P95 latency          0.055 ms                    â”‚
  â”‚ Throughput       23,809 classifications/sec      â”‚
  â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

## ğŸ¨ Custom Gestures

### JSON (Zero-Shot)

```json
{
  "gestures": [{
    "name": "gun",
    "fingers": { "thumb": "extended", "index": "extended", "middle": "curled", "ring": "curled", "pinky": "curled" },
    "constraints": [{ "type": "angle", "landmarks": [4, 0, 8], "min_angle": 30, "max_angle": 90 }]
  }]
}
```

### Train MLP (Higher Accuracy)

```python
from gesture_engine import GestureClassifier
classifier = GestureClassifier()
stats = classifier.train(X_landmarks, y_labels, epochs=100, save_path="model.pt")
```

## ğŸ“ Project Structure

```
src/gesture_engine/
â”œâ”€â”€ __init__.py        # Public API
â”œâ”€â”€ detector.py        # MediaPipe hand detection + normalization
â”œâ”€â”€ classifier.py      # Rule-based + MLP classification
â”œâ”€â”€ gestures.py        # Gesture definitions + registry
â”œâ”€â”€ pipeline.py        # Real-time pipeline with smoothing
â”œâ”€â”€ sequences.py       # Multi-gesture sequence detection
â”œâ”€â”€ recorder.py        # Record & replay gesture sessions
â””â”€â”€ server.py          # WebSocket streaming server (FastAPI)
examples/
â”œâ”€â”€ web_demo/          # Browser-based live demo
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ benchmark.py       # Performance measurement suite
â”œâ”€â”€ demo_webcam.py     # Live camera demo
â””â”€â”€ demo_collect.py    # Training data collection
tests/                 # pytest test suite
```

## ğŸ›  Installation

```bash
# Core (detection + classification)
pip install -e .

# With WebSocket server
pip install -e ".[server]"

# With ML training
pip install -e ".[train]"

# Everything
pip install -e ".[all]"
```

**Requirements:** Python 3.10+, a webcam (optional â€” use recordings for testing)

## API Reference

```python
from gesture_engine import (
    GesturePipeline,     # End-to-end: frame â†’ events
    HandDetector,        # MediaPipe landmark extraction
    GestureClassifier,   # Rule-based + MLP classification
    GestureRegistry,     # Gesture definition management
    SequenceDetector,    # Multi-gesture sequences
    GestureRecorder,     # Session recording
    GesturePlayer,       # Session replay
)
```

## License

[MIT](LICENSE)
